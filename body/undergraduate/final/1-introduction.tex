\cleardoublepage
\section{绪论}
\subsection{研究背景及意义}
在信息技术以前所未有的速度迭代演进、数字媒体日益渗透社会生活各个层面的时代背景下，视频凭借其多模态、高信息密度的特性，已无可争议地成为信息传播、娱乐互动及艺术表达的核心媒介与主导力量。近年来，深度学习浪潮席卷全球，极大地推动了人工智能生成内容（AIGC）技术的革命性进展。在此浪潮之中，视频生成（Video Generation）因其巨大的应用潜能与固有的技术挑战性——尤其是在保证内容的高保真度、时间连贯性与语义准确性方面——迅速成为学术界与工业界共同瞩目的焦点研究领域。

文本到视频（Text-to-Video, T2V）生成技术作为其中的关键分支，通过在海量文本-视频对数据上进行深度学习，已展现出令人振奋的能力，能够根据自然语言指令生成与之语义对应的视频片段。诸如Sora\cite{liu2024sora}、CogVideo\cite{yang2024cogvideox}、Kling\cite{kuaishou2024klingai}、Hunyuan\cite{sun2024hunyuan}以及Wanx\cite{wan2025}等前沿模型的相继问世，标志着T2V技术在生成视频的视觉质量、叙事连贯性以及内容多样性方面均取得了里程碑式的突破\cite{blattmann2023stable,singer2022make,blattmann2023align}，为内容创作领域开辟了全新的可能性。

然而，尽管T2V技术取得了显著成就，但单纯依赖文本提示作为唯一的条件输入，在实践中仍面临其固有的局限性。文本描述在传达复杂的视觉概念、精确的物体形态特征、微妙的动态交互以及独特的艺术风格时，往往受限于其信息密度不足和潜在的语义歧义性。例如，对于“一只奔跑的特定品种的狗，毛发在阳光下呈现特殊光泽”这样的描述，文本难以穷尽所有视觉细节，导致生成结果在具体形象、光影效果等方面与用户预期存在偏差。

相比之下，图像作为一种视觉优先的模态，能够承载远超文本的丰富视觉信息和精细入微的语义细节，包括物体的具体外观、纹理、光照、空间布局乃至抽象的艺术风格。因此，将图像信息作为额外的条件输入或核心视觉引导，与文本提示协同作用，被认为是进一步提升视频生成质量、增强精细可控性、实现高度个性化定制的关键突破路径。\
这种基于参考图像的视频生成（通常称为Ref-images Video Generation），有望克服纯文本驱动方法的不足，赋予生成模型更强的视觉锚定能力。特别地，在追求生成内容的高保真度（如精确复现特定物体或场景）、保持主体身份一致性（如在不同场景和动作中维持同一角色的外观特征）以及生成具有复杂动态和丰富细节的场景等高级应用需求下，如何设计有效机制以合理、高效地从参考图像中提取并利用其蕴含的视觉先验知识，便成为了一个亟待深入探索的核心科学问题与技术瓶颈。尽管基于参考图像的视频生成展现出巨大的应用前景和研究价值，但当前针对这一特定方向的系统性研究、成熟的理论框架以及稳健高效的方法论仍然相对匮乏，相关的基准数据集和评估体系亦有待完善。本文旨在应对这一挑战，深入探究图像条件视频生成的核心机制，以期推动该领域的发展。
\subsection{国内外研究现状与挑战}

基于扩散模型（Diffusion Models）\cite{ho2020denoising,song2020denoising,esser2024scaling}的生成范式，因其在图像生成领域展现出的卓越能力，已被自然地拓展应用于视频生成任务。通过在大规模文本-图像-视频对上进行训练，扩散模型在根据文本和图像输入生成高质量、具有自然运动的视频方面取得了巨大成功。如前所述，几大主流视频生成模型如CogVideo\cite{yang2024cogvideox}、Kling\cite{kuaishou2024klingai}、Hunyuan\cite{sun2024hunyuan}等模型，均展示了从文本描述和图像提示生成连贯、逼真视频序列的强大能力。这些技术的突破性进展，极大地激发了学术界和产业界对于根据用户定义的特定概念（如人物、物体、风格）进行个性化视频生成的浓厚兴趣，并催生了图像引导视频生成（I2V）作为 T2V 的重要发展方向。

在个性化视频生成的趋势下，研究者们开始尝试用不同的策略来引入额外的图像作为视觉指导，以期生成内容更可控、更定制化的视频。部分工作，例如ConceptMaster\cite{huang2025conceptmaster}在特定人物生成方面，以及一些风格化视频生成方法\cite{huang2024style}，已经能够在保持较高保真度的前提下，实现对参考图像特征的迁移。这些方法通常依赖于参数高效微调技术（Parameter-Efficient Fine-Tuning, PEFT），如LoRA（Low-Rank Adaptation），或者对整个模型进行微调，以期在生成视频中维持特定的身份或风格特征。然而，尽管上述方法取得了一定的进展，并在特定场景下展现了初步的图像控制能力，但在实际应用中，基于参考图像的视频生成技术仍面临诸多严峻挑战，导致生成效果往往难以完全满足用户预期，特别是在处理复杂场景、精细动态及高保真度要求时。现有方法\cite{kumari2023multi,ruiz2023dreambooth,wei2024dreamvideo,chen2023videodreamer,zhang2024attention,blattmann2023stable}在生成精确控制同一主体在不同位置、角度和光照条件下且符合文本描述的视频方面仍存在不足。

具体来说，目前图像引导视频生成还面临以下几个核心挑战：

（1）\textbf{语义控制与运动真实性不足。} 当文本提示要求主体执行特定、复杂的动作时，生成的视频往往无法准确响应文本指令，常常仅表现为无意义的整体平移、微小幅度的抖动，或与文本指令不符的简单姿态变化。尤其当需要根据文本指令调整主体的动作模式（例如，从“奔跑的小狗”转变为“跳舞的小狗”）或生成涉及复杂物理规律的运动时，许多模型难以生成精确有效且具有说服力的动作，生成的运动往往缺乏自然的物理惯性、力平衡等真实感建模，显得僵硬或不协调。这种缺陷在需要精细运动控制和高度逼真性的应用场景中尤为突出，严重制约了生成视频的实用价值。

（2）\textbf{主体特征保真度与跨帧一致性问题。} 在生成包含特定人物、角色或物体的视频时，如何确保这些主体在视频序列的多帧之间保持外观特征的高度一致性是一个重大挑战。现有方法容易出现主体特征失真，例如人物面部过度曝光、细节丢失，或在不同帧中主体外观发生漂移、甚至出现不合逻辑的特征重复（如多余的鼻子、肢体等）。这限制了生成视频在个性化内容创作、虚拟角色动画等领域的应用。

（3）\textbf{高分辨率生成效率与可扩展性瓶颈。} 囿于现有模型（尤其是基于扩散模型的SOTA方法，如Wan\cite{wan2025}, Hunyuan\cite{sun2024hunyuan}）固有的计算复杂度，生成高分辨率视频需要消耗巨大的计算时间和显存资源。单张高分辨率参考图引导的视频生成往往耗时漫长，且在处理更高分辨率或更长序列时容易出现显存不足（OOM）的情况。过长的生成时间和高昂的硬件要求极大地阻碍了AIGC技术走向商业化和市场化，限制了其在普通用户设备或云端大规模部署的可能性。

综上所述，虽然基于参考图像的视频生成技术展现了巨大的应用前景，但上述在主体保真度、语义可控性（特别是运动精度和真实性）以及高分辨率生成效率方面的挑战，表明当前技术距离生成高质量、可控、个性化视频的理想目标仍有较大差距。如何更有效地利用参考图像信息，设计出兼顾效率与性能的生成机制，突破这些瓶颈，是本文致力解决的核心问题。

\subsection{本文研究内容及创新点}
鉴于当前基于参考图像的视频生成技术所面临的挑战与局限性，本文的核心目的在于系统性地探索和改进参考图像信息的利用方式，以期在提升生成视频的整体质量、增强主体身份的一致性与动态表现的自然性、并兼顾高分辨率视频的生成效率等方面取得实质性进展。为此，本文将围绕以下三个核心方向展开探索与实验：

基于参考图像作为首帧的视频生成策略研究，深入分析将参考图像直接作为视频序列起始帧的生成范式。考察该策略对后续视频帧的内容一致性、主体特征保真度和连贯性的具体影响。通过对比不同信息注入方式和模型结构，探究其在保持参考图像核心内容与生成多样化动态之间的平衡机制。

参考图像身份（ID）信息注入的视频生成策略研究，针对现有方法在保持特定主体（如人物、动物）身份信息时出现的特征失真、形态僵硬等问题，提出并验证一种的ID信息注入机制。该机制旨在从参考图像中精准提取并有效融入核心身份特征至视频生成模型中，使得生成的视频能够在展现丰富动态和适应不同场景的同时，保持主体的身份一致性和视觉真实感。

面向给定高分辨率参考图像作为条件的视频生成策略研究，尝试在参考图像首帧引导的框架下，实现高效、高质量高分辨率视频生成的技术路径。重点关注如何在保证视觉细节丰富度和时间连贯性的同时，优化生成流程，降低计算资源消耗，寻求生成效率与最终视频质量的最佳平衡点。

在上述每一个研究方向下，本文均设计并实施了多种不同的方法尝试，并通过定性与定量的对比分析，评估各种策略的优缺点，以期为基于参考图像的可控视频生成技术发展提供有价值的见解和技术方案。

本文创新点体现在：针对视频生成内容不一致问题，提出了一种改进的身份（ID）保持视频生成方法，从而在增强动态表现力的同时，实现对主体身份的长期保持；为解决高分辨率视频生成的高开销问题，本文参考Flow matching流程，提出一种适用于加速视频推理的策略；为更好地评估生成视频的质量，本文从定性与定量两个维度，对不同设置下实验结果进行了多方面分析，并构建了一个包含108个视频文本描述涵盖人物、动物、物体和风景等多个类别的测试集。

总而言之，本文旨在深入探索参考图像在视频生成过程中的多维度应用，期望通过优化图像信息的融入方式，显著改善生成视频在内容一致性、身份保持、动态真实感以及高分辨率输出效率等方面的表现。这对于推动个性化内容创作、数字人合成、影视特效制作、数字艺术以及虚拟现实等领域的创新与发展具有重要的理论价值和实际应用前景。

\subsection{论文组织结构}
本论文共分为六章，具体内容安排如下：

第一章为绪论，本章首先阐述了文本到图像扩散生成模型的研究价值，指出该领域在生成质量、可控性等方面具有广阔的研究前景。随后，分析了当前参考图可控视频一致性生成任务中存在的主要问题，明确了其核心挑战，并进一步论述了本研究的实际意义与应用前景。接着，综述了国内外在参考图可控视频生成模型方面的研究进展，涵盖主要方法、技术演化及代表性成果。最后，概述了本文的研究内容与技术路线，并总结了本研究的主要贡献。

第二章为研究相关技术概述，本章介绍了与本文密切相关的关键技术，重点讲解了扩散模型（DiT）的基本原理及其在生成任务中的应用机制。同时，探讨了跨模态特征融合的方法，分析其在视频生成任务中的优势与挑战。此外，还详细介绍了时空位置编码技术RoPE在模型结构中的应用方式，阐明其在增强时序一致性方面的作用。

第三章为基于DiT模型的参考图可控视频一致性生成，本章围绕参考图可控视频生成这一核心问题，从三个方面逐步开展实验设计，旨在提高生成视频的一致性，并在保证生成质量的前提下，减少高分辨率参考图指导下的视频生成所需的总时长。首先介绍了本文所采用的实验架构和训练流程，并提供了关键流程的伪代码，便于理解与复现。随后，重点分析了固定首帧的参考图像到视频生成方法，通过“首帧替换”和“参考帧拼接”两种策略开展对比实验，并对结果进行深入分析。对于身份保持的参考图像视频生成任务，本文在上述框架基础上引入了灵活的身份信息保持机制，采用与参考帧拼接类似的策略实现更高的身份一致性。而在高分辨率参考图生成任务中，本文提出了一种基于流匹配的高效生成方法，有效降低了计算资源的消耗。

第四章为实验结果与分析，本章首先明确了实验评估指标，采用VBench为代表的多种图像与视频质量评估标准，并且逐一解释其作用及作用方式。在此基础上，分别从定性与定量两个角度，对参考图可控视频生成方法和高分辨率参考图生成方法的实验结果进行全面分析。其中，参考图可控生成部分涵盖两种实验设置。通过与现有SOTA方法的对比，系统评估了本文方法在视频一致性、图像质量和计算效率等方面的优劣，验证了所提方法的有效性与先进性。

第五章为总结与展望，本章对全文研究工作进行了总结，归纳了本文的核心研究内容、实验成果和主要结论。在肯定现阶段成果的同时，也指出了当前方法的局限性。最后，展望了未来可能的研究方向，包括探索多模态输入（如视频与音频的联合控制）、提升控制粒度与生成灵活性等，以进一步推动高质量、可控性强的视频生成技术的发展。

第六章为参考文献，本章汇总并列出了本文撰写过程中所引用的所有相关文献资料和研究工作，涵盖了文本到图像生成、扩散模型、跨模态融合、参考图可控视频生成、高分辨率生成方法等多个研究方向。
