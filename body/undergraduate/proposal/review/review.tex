\cleardoublepage
\newrefsection
\chapter{文献综述}

\section{背景介绍}
\par 近年来，扩散模型（Diffusion Models）在跨模态生成领域取得突破性进展，其核心突破体现在时空联合建模能力的显著提升。基于海量文本-视频对（T2V）的训练，代表性模型如Stable Video Diffusion\cite{blattmann2023stable}、Make-A-Video\cite{singer2022make}等通过级联时空模块架构，成功实现长达5秒以上的连贯视频生成，其关键创新在于将视频生成分解为空间域的特征保持和时间域的运动预测。这种技术突破催生了个性化视频生成的研究热潮，用户期望通过输入文本提示（Text Prompt）或参考图像（Reference Image），生成包含特定人物身份（如明星脸、历史人物）、定制化物品（如特定品牌产品）或动漫角色的动态内容。

当前研究主要沿三个技术路径推进：第一，基于大规模预训练的通用视频生成模型，如CogVideo\cite{hong2022cogvideo}、VideoGen\cite{li2023videogen}通过时空自注意力机制，在文本到视频的直接映射中实现语义可控性；第二，结合ControlNet\cite{zhang2023adding}等控制模块的增强方法，通过引入深度图、光流场等中间表示提升生成视频的空间布局和运动精度；第三，基于适配器（Adapter）或LoRA\cite{hu2022lora}的微调策略，如Tune-A-Video\cite{wu2023tune}通过在预训练模型中注入特定主体特征，实现个性化内容的生成。然而，现有方法在多概念组合生成时面临表征纠缠问题，当同时输入"戴眼镜的柯基犬冲浪"这类多属性提示时，模型常出现属性丢失或错位现象。

在个性化视频生成领域，核心挑战集中在身份保持（Identity Preservation）和时间一致性（Temporal Coherence）两个维度。研究显示，直接微调预训练模型会导致灾难性遗忘，而DreamBooth\cite{ruiz2023dreambooth}等基于文本反转（Text Inversion）的方法在视频生成中难以保持跨帧外观一致性。值得注意的是，Blattmann等\cite{blattmann2023stable}通过时空分层微调策略，在角色身份保持上取得进展，但其生成视频仍存在面部特征抖动问题。近期提出的Dynamicrafter\cite{xing2024dynamicrafter}将视频分解为外观锚点和运动潜码的双路建模，在特定人物生成中实现了90\%以上的身份相似度，但运动自然性仍有提升空间。

参考图引导生成的技术突破对影视制作、虚拟试穿等应用场景具有重要价值。在电影行业，Sora\cite{liu2024sora}等系统已能根据分镜脚本生成预可视化视频，但其角色一致性仍依赖大量镜头级微调。教育领域，历史场景还原需要精确保持人物服饰、建筑风格的时空一致性，现有方法在长视频生成中仍面临细节漂移问题。商业应用方面，Sun等\cite{sun2024outfitanyone}提出的虚拟试穿系统将服装参考图与人体姿态解耦，但在动态布料物理仿真方面尚待改进。这些实际需求推动着跨模态对齐、细粒度控制等关键技术的研究，如何构建兼顾生成质量与计算效率的个性化视频生成框架，仍是学术界与工业界共同关注的焦点。

\subsection{技术基础与发展}
\section{技术基础与发展}
扩散模型作为生成式人工智能的核心范式，其理论框架源于非平衡态热力学中的随机过程建模。Ho等人\cite{ho2020denoising}提出的去噪扩散概率模型（DDPM）通过参数化马尔可夫链实现渐进式去噪，奠定了现代扩散模型的理论基础。
Song等人\cite{song2020denoising}进一步提出基于分数匹配的SDE框架，将离散去噪过程扩展为连续时间域的随机微分方程求解，显著提升了生成效率。随着Rombach团队\cite{blattmann2023align}在潜在扩散模型（LDM）的规模化研究取得突破，图像生成质量达到可与人类绘画相媲美的水平。

视频生成作为时序数据的扩展任务，其核心挑战在于时空一致性的建模。CogVideo\cite{hong2022cogvideo}提出层次化视觉transformer架构，通过分阶段生成策略（16帧→32帧）实现视频分辨率从128×128到256×256的跨越。VideoLDM\cite{blattmann2023align}创新性地采用伪3D卷积模块，在空间卷积层中引入时间轴参数共享机制，使模型参数量控制在可训练范围内。最新进展中，Yang等人\cite{yang2024cogvideox}提出的多头扩散transformer（MMDiT）通过解耦时空注意力机制。当前技术发展已实现720p高清视频生成，时长从早期8帧扩展到5秒（49帧/81帧）。
\section{国内外研究现状}

\subsection{研究方向及进展}

现有研究在个性化视频生成领域主要利用两种方法。\textbf{基于身份图像重建的生成范式}：该方向致力于将文本到图像（T2I）模型的细粒度控制能力迁移至视频域。核心思路是通过跨模态对齐实现参考图像的身份特征保持。以DreamBooth\cite{ruiz2023dreambooth}和Custom Diffusion\cite{kumari2023multi}为代表的参数调优方法，通过轻量化微调（如LoRA）将目标概念嵌入模型参数空间。Huang等\cite{huang2025conceptmaster}提出的ConceptMaster系统引入概念解纠缠损失函数，在MS-COCO数据集上实现多对象组合生成。此类方法虽能保持高保真度），但存在运动建模能力受限的问题，生成视频中大多数样本仅呈现平移或缩放等简单运动。
通过可插拔模块扩展预训练模型的语义控制维度。ConsisID\cite{huang2024consistentid}等则采用Ip-Adapter类似思想设计分层特征注入网络，其跨帧注意力一致性损失使身份保持率显著提升。Zhao等\cite{zhao2024motiondirector}提出的MotionDirector创新性地解耦外观与运动建模。最新进展中，PhysDiff\cite{yuan2023physdiff}通过刚体动力学约束将物理合理性指标显著提升，而StreamDiffusion\cite{kodaira2023streamdiffusion}的残差传播机制实现视频加速生成。\textbf{基于注意力交互的生成范式}：该方向聚焦于跨模态特征的动态融合机制，突破传统级联式生成的局限。特别的有时空注意力机制，CogVideoX\cite{yang2024cogvideox}提出多头扩散transformer架构，其时空解耦注意力公式为：
\begin{equation}
\text{Attn}(Q,K,V) = \text{Softmax}(\frac{Q_sK_s^T}{\sqrt{d_k}} \otimes \frac{Q_tK_t^T}{\sqrt{d_k}})V
\end{equation}此外分阶段生成策略对于生成高质量图像视频也至关重要，CharacterGLM\cite{zhou2023characterglm}构建三级生成流水线范式，并取得了较高的场景连贯性结果， 助力于电影级视频生成任务。
\subsection{存在问题}
当前个性化视频生成技术在实际应用中仍面临若干关键性挑战，主要体现在语义控制精度与运动建模能力两个维度：

\textbf{个性化定制局限}：现有方法在处理多概念组合提示时普遍存在语义失准现象。当输入提示涉及多个交互对象（如"草原上进食胡萝卜的白兔"）时，生成结果中频繁出现对象特征与背景环境的空间逻辑矛盾。典型问题表现为前景主体与背景元素的纹理异常融合（如动物毛发与植被纹理的交叉污染），\
以及细粒度属性难以保持（如特定品种宠物的瞳色与毛色偏差）。此外，在开放域概念组合场景下，系统往往无法准确解析文本中隐含的层次化语义关系，导致生成主体出现身份特征漂移。

\textbf{运动生成瓶颈}：在长时序视频合成任务中，现有模型对复杂运动模式的建模能力显著受限。一方面，动态对象的肢体结构在连续帧中易产生渐进式形变，尤其在关节部位与非刚性物体（如衣物褶皱、流体运动）的轨迹预测中，普遍存在运动学合理性缺失问题。\
另一方面，当需根据文本指令调整主体动作模式时（如将"奔跑的小狗"改为"跳舞的小狗"），生成结果往往仅呈现简单的运动幅度调整，而缺乏对物理规律（如质心转移、惯性作用）的准确建模，导致动作真实性不足。此类缺陷在需要精细运动控制的场景中尤为突出，严重制约了生成视频的实用价值。

\section{研究展望}
本工作提出\textbf{Character Animator}框架，通过系统化架构创新突破现有视频生成技术在时空一致性控制方面的瓶颈。该框架包含以下核心技术演进：
本研究围绕多模态驱动视频生成的核心挑战，构建了从特征融合到时空一致性控制的完整技术体系，重点突破以下三个研究方向：

\textbf{跨模态特征融合架构}：设计多模态混合编码器，通过三维视觉压缩器与文本编码器的联合优化，实现图像、视频和文本输入的协同表征。该架构采用分层注意力机制，在潜空间建立跨模态特征交互通道，支持多源输入信息的动态加权融合。特别地，针对参考图像与文本提示的语义对齐问题，提出可学习的模态门控机制，自动调节不同输入源的贡献权重。

\begin{figure}[htbp]
    \centering
    \begin{tikzcd}[column sep=small, row sep=large, nodes={align=center}]
        & \text{多模态输入} \arrow[d] \\
        & \text{特征提取器} \arrow[dl, shift right=1ex] \arrow[dr, shift left=1ex] \\
        \text{文本编码器} \arrow[dr] & & \text{视觉编码器} \arrow[dl] \\
        & \text{时空融合模块} \arrow[d] \\
        & \text{扩散主干网络} \arrow[d] \\
        & \text{视频解码器}
    \end{tikzcd}
    \caption{CharacterAnimator总体架构}
    \label{fig:architecture2}
\end{figure}
\textbf{时空一致性建模机制}：提出双路时空控制策略，通过时序分组生成模块解耦内容生成与运动预测过程。在空间维度构建动态身份保持网络，采用记忆增强式残差连接实现主体特征跨帧传播；在时间维度设计运动轨迹预测器，基于物理启发的运动先验建模实现自然动作合成。引入对抗正则化约束，在训练阶段强化生成序列的时空连续性。

\textbf{参考引导生成范式}：创新性地提出组合式微调框架，将参考图像特征深度嵌入视频生成流程。通过构建跨帧特征传播链，建立参考图像关键属性（如人物身份、物体形态）与生成视频内容的强关联。如图\ref{data}所示，该系统能够以单张参考图像为引导，结合多样化文本提示生成主体一致而内容各异的视频片段，为商业视频制作中的多场景内容生成提供技术支持。

一言以蔽之，本研究提出基于参考图像引导的动态视频生成框架，通过跨模态融合机制将视觉特征与文本语义深度耦合，实现主体一致的多变体视频创作。如图\ref{fig:data}所示，系统以单张参考图像为视觉锚点，结合差异化文本提示生成多段主体身份高度统一、场景动作各异的视频序列，\
其核心突破在于构建特征解耦重组策略：采用分层注意力机制分离主体身份编码与动态行为编码，通过动态特征路由网络实现身份特征的跨帧稳定传递，同时允许动作语义的灵活重组。在商业应用层面，该技术可支撑单张产品图生成多角度演示视频，或基于角色设定图自动创作系列剧情片段，显著提升电商视频、品牌宣传视频等内容的生产效率，为"一源多用"的视频工业化生产提供技术支持。
\subsection{技术路线}

\subsubsection{总体架构}
如图\ref{fig:architecture2}所示，本框架构建多级特征处理pipeline：首先通过并行编码分支（三维视觉压缩器vae和文本编码器text tokenizer）对文本、图像及视频输入进行模态特征提取，\
随后在潜空间进行跨模态特征融合。时空融合模块采用层级注意力机制建立跨模态关联，扩散模型网络通过迭代去噪过程实现特征到视频的映射，最终由视频解码器重建高保真输出序列。
该框架预期实现支持参考图像定制化生成，且生成视频主体明确，动作符合文本提示词要求; 生成支持多模态输入的视频，在接受文本提示词的同时可以接受一张或多张参考图像或者视频。

\newpage
\begingroup
    \linespreadsingle{}
    \printbibliography[title={参考文献}]
\endgroup